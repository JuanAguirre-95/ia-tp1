{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion\n",
    "\n",
    "Dado un set de datos de accidentes automovilisticos se quieren utilizar tecnicas de Machine Learning para entrenar un modelo y poder predecir o clasificar la severidad de un accidente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tipo de problema\n",
    "\n",
    "Se desea entrenar el modelo para determinar la severidad de un accidente. La severidad se clasifica entre los valores 1 a 4, siendo 1 el menos severo y 4 el más severo. Dado que debemos clasificar el set de datos con valores discretos lo mas conveniente seria utilizar modelos de clasificacion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables y caracteristicas\n",
    "\n",
    "Interpretar las variables del dataset y como pueden servirnos o no para la estimacion. Por que elegimos las columnas, que nuevas columnas podemos crear a partir de las que tenemos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocesamiento del set de datos\n",
    "\n",
    "En este paso se determina el estado del dataset, que datos nos sirven y que se puede mejorar para que el entrenamiento sea satisfactorio.\n",
    "\n",
    "Ver tp1_dataset_processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(933236, 176)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Columns: 176 entries, index to Sunrise_Sunset_Night\n",
      "dtypes: bool(13), float64(9), int64(4), timedelta64[ns](1), uint8(149)\n",
      "memory usage: 26.1 MB\n"
     ]
    }
   ],
   "source": [
    "#%store -r dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_feather(\"dataframe\")\n",
    "print(dataset.shape)\n",
    "dataset = dataset[0:100000] #Para que no me explote la pc no cargo todo el dataset\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierto el tiempo a horas para que quede mas comodo\n",
    "dataset[\"Time_Elapsed\"] = dataset[\"Time_Elapsed\"].astype(np.timedelta64) / np.timedelta64(1, 'h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de prediccion a aplicar\n",
    "\n",
    "Aca entraria una breve explicacion de que modelos vamos a usar y el por que\n",
    "\n",
    "* Formalizar tecnica de seleccion de datos\n",
    "* Evaluar modelos usados segun resultado obtenido\n",
    "* Comentar ventajas y desventajas de los modelos elegidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.linear_model import LinealRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset #Features a entrenar\n",
    "y = dataset.pop(\"Severity\") #Datos de target\n",
    "\n",
    "#Funcion auxiliar para escalar y dividir los datos\n",
    "def scale_data(X,y):\n",
    "    #Separamos el dataset en partes de entrenamiento y de testeo\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,y)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    #Entrenamos el scaler con los datos del set de entrenamiento\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    #Aplicamos la transformacion a ambos sets de entrenamiento y de testeo\n",
    "    norm_x_train = scaler.transform(X_train)\n",
    "    norm_x_test = scaler.transform(X_test)\n",
    "    \n",
    "    return norm_x_train, norm_x_test, Y_train, Y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para escalar los datos usamos MinMaxScaler, entrenado con la particion de entrenamiento. Esto es porque en la vida real la particion de test no la conocemos hasta tratar de predecir con un modelo ya entrenado. Por eso hacemos fit sobre X_train y transform en ambos X_train y X_test, logrando simular la realidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 1: SGD (Stochastic Gradient Descent)\n",
    "\n",
    "SGD enfoque simple y muy eficiente para adaptarse a modelos lineales. Es particularmente útil cuando el número de muestras es muy grande. Admite diferentes funciones de pérdida y penalizaciones por clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Starting...\n",
      "Scaling done. Durarion: 3.9\n",
      "Sgd training starting...\n",
      "fit done with average 1. Duration: 3.11\n",
      "Sgd training starting...\n",
      "fit done with average 2. Duration: 2.94\n",
      "Sgd training starting...\n"
     ]
    }
   ],
   "source": [
    "#Escalamos los datos\n",
    "print(\"Scaling Starting...\")\n",
    "start_time = time.time()\n",
    "norm_x_train, norm_x_test, y_train, y_test = scale_data(X,y)\n",
    "end_time = time.time()\n",
    "print(f\"Scaling done. Durarion: {round(end_time-start_time,2)}\")\n",
    "\n",
    "i_range = range(1,50)\n",
    "sgd_train_score = []\n",
    "sgd_test_score = []\n",
    "for i in i_range:\n",
    "    print(\"Sgd training starting...\")\n",
    "    start_time = time.time()\n",
    "    est = SGDClassifier(alpha=0.002,average=i)\n",
    "    est.fit(norm_x_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"fit done with average {i}. Duration: {round(end_time-start_time,2)}\")\n",
    "    #y_pred = est.predict(norm_x_test)\n",
    "    #scores[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #list_scores.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    sgd_train_score.append(est.score(norm_x_train, y_train))\n",
    "    sgd_test_score.append(est.score(norm_x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(i_range, sgd_train_score, label=\"Scores de set train\")\n",
    "plt.plot(i_range, sgd_test_score, label=\"Scores de set test\")\n",
    "plt.xlabel(\"Valores para i\")\n",
    "plt.ylabel(\"Resultado de accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si elegimos el el valor promedio a partir de 22 y 35 obtenemos la mejor relacion entre overfitting y underfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 2: Clasificador KNN\n",
    "\n",
    "La clasificación basada en vecinos es un tipo de aprendizaje perezoso, ya que no intenta construir un modelo interno general, sino que simplemente almacena instancias de los datos de entrenamiento. La clasificación se calcula a partir de un voto de mayoría simple de los k vecinos más cercanos de cada punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "print(\"Scaling Starting...\")\n",
    "start_time = time.time()\n",
    "norm_x_train, norm_x_test, y_train, y_test = scale_data(X,y)\n",
    "end_time = time.time()\n",
    "print(f\"Scaling done. Durarion: {round(end_time-start_time,2)}\")\n",
    "\n",
    "#Definimos un rango de Ks para probar\n",
    "k_range = range(1,21)\n",
    "\n",
    "#Nos guardamos los scores de entrenamiento para graficar luego.\n",
    "knn_train_score = []\n",
    "knn_test_score = []\n",
    "\n",
    "for k in k_range: #Automaticamente testeamos el modelo para k entre 1 y k_range, y nos guardamos el accuracy de cada uno\n",
    "    print(\"Knn Starting...\",k)\n",
    "    start_time = time.time()\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(norm_x_train,y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Knn with k = {k} done. Duration: {round(end_time-start_time,2)}\")\n",
    "    #y_pred = knn.predict(norm_x_test)\n",
    "    #print(\"Pred done\")\n",
    "    knn_train_score.append(knn.score(norm_x_train, y_train))\n",
    "    knn_test_score.append(knn.score(norm_x_test, y_test))\n",
    "    print(f\"Done with {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(k_range, knn_train_score, label=\"Scores de set train\")\n",
    "plt.plot(k_range, knn_test_score, label=\"Scores de set test\")\n",
    "plt.xlabel(\"Valores para k\")\n",
    "plt.ylabel(\"Resultado de accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 3: Clasificador MLP (Multi-layered Perceptron)\n",
    "\n",
    "Una red neuronal es un conjunto de neuronas (funciones de activación) en capas que se procesan secuencialmente para relacionar una entrada con una salida. Este ejemplo implementa un algoritmo de perceptrón multicapa (MLP) donde probamos los distintos algoritmos de resolucion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "print(\"Scaling Starting...\")\n",
    "start_time = time.time()\n",
    "norm_x_train, norm_x_test, y_train, y_test = scale_data(X,y)\n",
    "end_time = time.time()\n",
    "print(f\"Scaling done. Durarion: {round(end_time-start_time,2)}\")\n",
    "\n",
    "mlp_train_score = []\n",
    "mlp_test_score = []\n",
    "\n",
    "solvers = [\"lbfgs\",\"sgd\", \"adam\"]\n",
    "for solver in solvers:\n",
    "    print(f\"Starting training with {solver}\")\n",
    "    start_time = time.time()\n",
    "    clf = MLPClassifier(solver='lbfgs',alpha=1e-5,max_iter=10000,activation='relu',hidden_layer_sizes=(10,50,10), random_state=1, shuffle=True)\n",
    "    clf.fit(norm_x_train,y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished training with {solver}. Duration: {round(end_time-start_time,2)}\")\n",
    "    #y_pred = clf.predict(norm_x_test)\n",
    "    mlp_train_score.append(clf.score(norm_x_train,y_train))\n",
    "    mlp_test_score.append(clf.score(norm_x_test,y_test))\n",
    "    print(f\"Done with {solver}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(solvers, mlp_train_score, label=\"Scores de set train\")\n",
    "plt.bar(solvers, mlp_test_score, label=\"Scores de set test\")\n",
    "plt.xlabel(\"Solver\")\n",
    "plt.ylabel(\"Resultado de accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentacion de los resultados obtenidos\n",
    "\n",
    "Aca mostrar resultados de las predicciones, graficos o lo que sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
